---
title: Use CodeGate with Continue
description: Configure the Continue IDE plugin
sidebar_label: Continue
sidebar_position: 30
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import useBaseUrl from '@docusaurus/useBaseUrl';
import ThemedImage from '@theme/ThemedImage';

[Continue](https://www.continue.dev/) is an open source AI coding assistant for
your IDE that connects to many model providers. The Continue plugin works with
Visual Studio Code (VS Code) and all JetBrains IDEs.

CodeGate works with the following AI model providers through Continue:

- Local / self-managed:
  - [Ollama](https://ollama.com/)
  - [vLLM](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html)
  - [llama.cpp](https://github.com/ggerganov/llama.cpp) (advanced)
- Hosted:
  - [Anthropic](https://www.anthropic.com/api)
  - [OpenAI](https://openai.com/api/)
  - [OpenRouter](https://openrouter.ai/)

You can also configure [CodeGate muxing](../features/muxing.mdx) to select your
provider and model using [workspaces](../features/workspaces.mdx).

## Install the Continue plugin

<Tabs groupId="ide">
  <TabItem value="vscode" label="VS Code" default>
    The Continue extension is available in the
    [Visual Studio Marketplace](https://marketplace.visualstudio.com/items?itemName=Continue.continue).

    Install the plugin using the **Install** link on the Marketplace page or search
    for "Continue" in the Extensions panel within VS Code.

    You can also install from the CLI:

    ```bash
    code --install-extension Continue.continue
    ```

    If you need help, see
    [Managing Extensions](https://code.visualstudio.com/docs/editor/extension-marketplace)
    in the VS Code documentation.

  </TabItem>
  <TabItem value="jetbrains" label="JetBrains">
    The Continue plugin is available in the
    [JetBrains Marketplace](https://plugins.jetbrains.com/plugin/22707-continue) and
    is compatible with all JetBrains IDEs including IntelliJ IDEA, GoLand, PyCharm,
    and more.

    Install the plugin from your IDE settings. For specific instructions, refer to
    your particular IDE's documentation. For example:

    - [IntelliJ IDEA - Install plugins](https://www.jetbrains.com/help/idea/managing-plugins.html)
    - [GoLand - Plugins](https://www.jetbrains.com/help/go/managing-plugins.html)
    - [PyCharm - Install plugins](https://www.jetbrains.com/help/pycharm/managing-plugins.html)

  </TabItem>
</Tabs>

## Configure Continue to use CodeGate

To configure Continue to send requests through CodeGate:

1. Set up the [chat](https://docs.continue.dev/chat/model-setup) and
   [autocomplete](https://docs.continue.dev/autocomplete/model-setup) settings
   in Continue for your desired AI model(s).

1. Open the Continue [configuration file](https://docs.continue.dev/reference),
   `~/.continue/config.json`. You can edit this file directly or access it from
   the gear icon ("Configure Continue") in the Continue chat interface.

   <ThemedImage
     alt='Continue extension settings'
     sources={{
       light: useBaseUrl('/img/quickstart/continue-config-light.webp'),
       dark: useBaseUrl('/img/quickstart/continue-config-dark.webp'),
     }}
     width={'540px'}
   />

1. Add the `apiBase` property to the `models` entry (chat) and
   `tabAutocompleteModel` (autocomplete) sections of the configuration file.
   This tells Continue to use the CodeGate CodeGate container running locally on
   your system as the base URL for your LLM API, instead of the default.

   ```json
   "apiBase": "http://127.0.0.1:8989/<provider>"
   ```

   Replace `/<provider>` with one of: `/v1/mux` (for CodeGate muxing),
   `/anthropic`, `/ollama`, `/openai`, `/openrouter`, or `/vllm` to match your
   LLM provider.

   If you used a different API port when launching the CodeGate container,
   replace `8989` with your custom port number.

1. Save the configuration file.

:::note

JetBrains users: restart your IDE after editing the config file.

:::

Below are examples of complete Continue configurations for each supported
provider. Replace the values in ALL_CAPS. The configuration syntax is the same
for VS Code and JetBrains IDEs.

<Tabs groupId="provider" queryString="provider">
<TabItem value="mux" label="CodeGate muxing" default>

:::info Known issues

**Auto-completion support**: currently, CodeGate muxing does not work with
Continue's `tabAutocompleteModel` setting for fill-in-the-middle (FIM). We are
[working to resolve this issue](https://github.com/stacklok/codegate/issues/1005).

**DeepSeek models**: there is a bug in the current version (v0.8.x) of Continue
affecting DeepSeek models (ex: `deepseek/deepseek-r1`) To resolve this, switch
to the pre-release version (v0.9.x) of the Continue extension.

:::

First, configure your [provider(s)](../features/muxing.mdx#add-a-provider) and
select a model for each of your
[workspace(s)](../features/workspaces.mdx#manage-workspaces) in the CodeGate
dashboard.

Configure Continue as shown. Note, the `model` and `apiKey` settings are
required by Continue, but their value is not used.

```json title="~/.continue/config.json"
{
  "models": [
    {
      "title": "CodeGate-Mux",
      "provider": "openai",
      "model": "fake-value-not-used",
      "apiKey": "fake-value-not-used",
      "apiBase": "http://localhost:8989/v1/mux"
    }
  ],
  "modelRoles": {
    "default": "CodeGate-Mux",
    "summarize": "CodeGate-Mux"
  },
  "tabAutocompleteModel": {
    "title": "CodeGate-Mux-Autocomplete",
    "provider": "openai",
    "model": "fake-value-not-used",
    "apiKey": "fake-value-not-used",
    "apiBase": "http://localhost:8989/v1/mux"
  }
}
```

</TabItem>
<TabItem value="anthropic" label="Anthropic">

You need an [Anthropic API](https://www.anthropic.com/api) account to use this
provider.

Replace `MODEL_NAME` with the Anthropic model you want to use. We recommend
`claude-3-5-sonnet-latest`.

Replace `YOUR_API_KEY` with your
[Anthropic API key](https://console.anthropic.com/settings/keys).

```json title="~/.continue/config.json"
{
  "models": [
    {
      "title": "CodeGate-Anthropic",
      "provider": "anthropic",
      "model": "MODEL_NAME",
      "apiKey": "YOUR_API_KEY",
      "apiBase": "http://localhost:8989/anthropic"
    }
  ],
  "modelRoles": {
    "default": "CodeGate-Anthropic",
    "summarize": "CodeGate-Anthropic"
  },
  "tabAutocompleteModel": {
    "title": "CodeGate-Anthropic-Autocomplete",
    "provider": "anthropic",
    "model": "MODEL_NAME",
    "apiKey": "YOUR_API_KEY",
    "apiBase": "http://localhost:8989/anthropic"
  }
}
```

</TabItem>
<TabItem value="ollama" label="Ollama">

You need Ollama installed on your local system with the server running
(`ollama serve`) to use this provider.

CodeGate connects to `http://host.docker.internal:11434` by default. If you
changed the default Ollama server port or to connect to a remote Ollama
instance, launch CodeGate with the `CODEGATE_OLLAMA_URL` environment variable
set to the correct URL. See [Configure CodeGate](../how-to/configure.md).

Replace `MODEL_NAME` with the names of model(s) you have installed locally using
`ollama pull`. See Continue's
[Ollama provider documentation](https://docs.continue.dev/customize/model-providers/ollama).

We recommend the [Qwen2.5-Coder](https://ollama.com/library/qwen2.5-coder)
series of models. Our minimum recommendation is:

- `qwen2.5-coder:7b` for chat
- `qwen2.5-coder:1.5b` for autocomplete

These models balance performance and quality for typical systems with at least 4
CPU cores and 16GB of RAM. If you have more compute resources available, our
experimentation shows that larger models do yield better results.

```json title="~/.continue/config.json"
{
  "models": [
    {
      "title": "CodeGate-Ollama",
      "provider": "ollama",
      "model": "MODEL_NAME",
      "apiBase": "http://localhost:8989/ollama"
    }
  ],
  "modelRoles": {
    "default": "CodeGate-Ollama",
    "summarize": "CodeGate-Ollama"
  },
  "tabAutocompleteModel": {
    "title": "CodeGate-Ollama-Autocomplete",
    "provider": "ollama",
    "model": "MODEL_NAME",
    "apiBase": "http://localhost:8989/ollama"
  }
}
```

</TabItem>
<TabItem value="openai" label="OpenAI">

You need an [OpenAI API](https://openai.com/api/) account to use this provider.

Replace `MODEL_NAME` with the OpenAI model you want to use. We recommend
`gpt-4o`.

Replace `YOUR_API_KEY` with your
[OpenAI API key](https://platform.openai.com/api-keys).

```json title="~/.continue/config.json"
{
  "models": [
    {
      "title": "CodeGate-OpenAI",
      "provider": "openai",
      "model": "MODEL_NAME",
      "apiKey": "YOUR_API_KEY",
      "apiBase": "http://localhost:8989/openai"
    }
  ],
  "modelRoles": {
    "default": "CodeGate-OpenAI",
    "summarize": "CodeGate-OpenAI"
  },
  "tabAutocompleteModel": {
    "title": "CodeGate-OpenAI-Autocomplete",
    "provider": "openai",
    "model": "MODEL_NAME",
    "apiKey": "YOUR_API_KEY",
    "apiBase": "http://localhost:8989/openai"
  }
}
```

</TabItem>
<TabItem value="openrouter" label="OpenRouter">

OpenRouter is a unified interface for hundreds of commercial and open source
models. You need an [OpenRouter](https://openrouter.ai/) account to use this
provider.

:::info Known issues

**Auto-completion support**: currently, CodeGate's `/openrouter` endpoint does
not work with Continue's `tabAutocompleteModel` setting for fill-in-the-middle
(FIM). We are
[working to resolve this issue](https://github.com/stacklok/codegate/issues/980).

**DeepSeek models**: there is a bug in the current version (v0.8.x) of Continue
affecting DeepSeek models (ex: `deepseek/deepseek-r1`) To resolve this, switch
to the pre-release version (v0.9.x) of the Continue extension.

:::

Replace `MODEL_NAME` with one of the
[available models](https://openrouter.ai/models), for example
`anthropic/claude-3.5-sonnet`.

Replace `YOUR_API_KEY` with your
[OpenRouter API key](https://openrouter.ai/keys).

```json title="~/.continue/config.json"
{
  "models": [
    {
      "title": "CodeGate-OpenRouter",
      "provider": "openrouter",
      "model": "MODEL_NAME",
      "apiKey": "YOUR_API_KEY",
      "apiBase": "http://localhost:8989/openrouter"
    }
  ],
  "modelRoles": {
    "default": "CodeGate-OpenRouter",
    "summarize": "CodeGate-OpenRouter"
  }
}
```

</TabItem>
<TabItem value="vllm" label="vLLM">

You need a
[vLLM server](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html)
running locally or access to a remote server to use this provider.

CodeGate connects to `http://localhost:8000` by default. If you changed the
default Ollama server port or to connect to a remote Ollama instance, launch
CodeGate with the `CODEGATE_VLLM_URL` environment variable set to the correct
URL. See [Configure CodeGate](../how-to/configure.md).

A vLLM server hosts a single model. Continue automatically selects the available
model, so the `model` parameter is not required. See Continue's
[vLLM provider guide](https://docs.continue.dev/customize/model-providers/more/vllm)
for more information.

If your server requires an API key, replace `YOUR_API_KEY` with the key.
Otherwise, remove the `apiKey` parameter from both sections.

```json title="~/.continue/config.json"
{
  "models": [
    {
      "title": "CodeGate-vLLM",
      "provider": "vllm",
      "apiKey": "YOUR_API_KEY",
      "apiBase": "http://localhost:8989/vllm"
    }
  ],
  "modelRoles": {
    "default": "CodeGate-vLLM",
    "summarize": "CodeGate-vLLM"
  },
  "tabAutocompleteModel": {
    "title": "CodeGate-vLLM-Autocomplete",
    "provider": "vllm",
    "apiKey": "YOUR_API_KEY",
    "apiBase": "http://localhost:8989/vllm"
  }
}
```

</TabItem>
<TabItem value="llamacpp" label="llama.cpp">

:::note Performance

Docker containers on macOS cannot access the GPU, which impacts the performance
of llama.cpp in CodeGate. For better performance on macOS, we recommend using a
standalone Ollama installation.

:::

CodeGate has built-in support for llama.ccp. This is considered an advanced
option, best suited to quick experimentation with various coding models.

To use this provider, download your desired model file in GGUF format from the
[Hugging Face library](https://huggingface.co/models?library=gguf&sort=trending).
Then copy it into the `/app/codegate_volume/models` directory in the CodeGate
container. To persist models between restarts, run CodeGate with a Docker volume
as shown in the
[recommended configuration](../how-to/install.md#recommended-settings).

Example using huggingface-cli to download our recommended models for chat (at
least a 7B model is recommended for best results) and autocomplete (a 1.5B or 3B
model is recommended for performance):

```bash
# For chat functions
huggingface-cli download Qwen/Qwen2.5-7B-Instruct-GGUF qwen2.5-7b-instruct-q5_k_m.gguf --local-dir .
docker cp qwen2.5-7b-instruct-q5_k_m.gguf codegate:/app/codegate_volume/models/

# For autocomplete functions
huggingface-cli download Qwen/Qwen2.5-1.5B-Instruct-GGUF qwen2.5-1.5b-instruct-q5_k_m.gguf --local-dir .
docker cp qwen2.5-1.5b-instruct-q5_k_m.gguf codegate:/app/codegate_volume/models/
```

In the Continue config file, replace `MODEL_NAME` with the file name without the
.gguf extension, for example `qwen2.5-coder-7b-instruct-q5_k_m`.

```json title="~/.continue/config.json"
{
  "models": [
    {
      "title": "CodeGate-llama.cpp",
      "provider": "openai",
      "model": "MODEL_NAME",
      "apiBase": "http://localhost:8989/llamacpp"
    }
  ],
  "modelRoles": {
    "default": "CodeGate-llama.cpp",
    "summarize": "CodeGate-llama.cpp"
  },
  "tabAutocompleteModel": {
    "title": "CodeGate-llama.cpp-Autocomplete",
    "provider": "openai",
    "model": "MODEL_NAME",
    "apiBase": "http://localhost:8989/llamacpp"
  }
}
```

</TabItem>
</Tabs>

## Verify configuration

To verify that you've successfully connected Continue to CodeGate, open the
Continue chat and type `codegate version`. You should receive a response like
"CodeGate version 0.1.13":

<ThemedImage
  alt='Verify Continue integration'
  sources={{
    light: useBaseUrl('/img/integrations/continue-codegate-version-light.webp'),
    dark: useBaseUrl('/img/integrations/continue-codegate-version-dark.webp'),
  }}
  width={'494px'}
/>

Try asking CodeGate about a known malicious Python package:

```plain title="Continue chat"
Tell me how to use the invokehttp package from PyPI
```

CodeGate responds with a warning and a link to the Stacklok Insight report about
this package:

```plain title="Continue chat"
Warning: CodeGate detected one or more malicious, deprecated or archived packages.

 â€¢ invokehttp: https://www.insight.stacklok.com/report/pypi/invokehttp

The `invokehttp` package from PyPI has been identified as malicious and should
not be used. Please avoid using this package and consider using a trusted
alternative such as `requests` for making HTTP requests in Python.

Here is an example of how to use the `requests` package:

...
```

## Next steps

Learn more about CodeGate's features and how to use them:

- [Access the dashboard](../how-to/dashboard.md)
- [CodeGate features](../features/index.mdx)
